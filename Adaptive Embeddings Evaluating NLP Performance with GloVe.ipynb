{"cells":[{"cell_type":"markdown","id":"4a4684dd-ea82-4d9d-b1cf-1281cf7e5c29","metadata":{},"source":["---\n"]},{"cell_type":"markdown","id":"5ee5b82c-8ec6-404e-9439-1a66347d3406","metadata":{},"source":["The following required libraries are __not__ pre-installed in the Skills Network Labs environment. __You will need to run the following cell__ to install them:\n"]},{"cell_type":"code","execution_count":null,"id":"f6547eac-eb72-43e8-b274-3b7c22c449d8","metadata":{},"outputs":[],"source":["!pip install --user --upgrade -U torch==2.2.0 torchtext==0.17.0 torchdata==0.7.1 portalocker==2.8.2\n","!pip install -U pandas==2.2.2\n","!pip install -U matplotlib==3.8.4\n","!pip install --user -U scikit-learn==1.4.2\n","!pip install --user -U plotly==5.22.0\n","!pip install -U numpy==1.26.4"]},{"cell_type":"markdown","id":"b48162c6-a39b-49bf-abda-cbdd5969000e","metadata":{},"source":["After the installation of libraries is completed, restart your kernel. You can do this by running the code below.\n"]},{"cell_type":"code","execution_count":null,"id":"27ad06d1-2edd-49f0-a6d3-af2f1059304e","metadata":{},"outputs":[],"source":["import os\n","os._exit(00)"]},{"cell_type":"markdown","id":"cc68dd2b-653b-41d4-a2bf-16d87c0854a3","metadata":{},"source":["If, for some reason, the above code did not work, you can restart the kernel by clicking the **Restart the kernel** icon.\n"]},{"cell_type":"markdown","id":"0875ba80-5ba8-44cb-87f0-ae35a424df29","metadata":{},"source":["### Importing Required Libraries\n","\n","Run the following code to import the required libraries.\n"]},{"cell_type":"code","execution_count":null,"id":"0c4799dc-dde4-4f72-bdde-6e2995b682e1","metadata":{},"outputs":[],"source":["from tqdm import tqdm\n","import numpy as np\n","import pandas as pd\n","from itertools import accumulate\n","import matplotlib.pyplot as plt\n","import math\n","\n","import torch\n","import torch.nn as nn\n","from torchtext.vocab import build_vocab_from_iterator, GloVe, Vectors\n","\n","from sklearn.manifold import TSNE\n","\n","from torch.utils.data import DataLoader\n","import numpy as np\n","from torchtext.datasets import AG_NEWS\n","from IPython.display import Markdown as md\n","from tqdm import tqdm\n","\n","from torch.utils.data.dataset import random_split\n","from torchtext.data.functional import to_map_style_dataset\n","from sklearn.manifold import TSNE\n","import plotly.graph_objs as go\n","\n","# You can also use this section to suppress warnings generated by your code:\n","def warn(*args, **kwargs):\n","    pass\n","import warnings\n","warnings.warn = warn\n","warnings.filterwarnings('ignore')\n","\n","import pickle"]},{"cell_type":"markdown","id":"0ac62f36-204c-4c4a-a003-6955d93635be","metadata":{},"source":["### Defining Helper Functions\n","\n","The helper functions defined below are designed to improve the readability of the code that follows. These functions are primarily used for graph plotting and file operations, such as saving and loading data. It's important to note that these helper functions are not the primary focus of this lab, so you don't need to spend too much time on them. Run the subsequent cells to load these helper functions.\n"]},{"cell_type":"code","execution_count":null,"id":"396040fa-402f-4649-ac89-7e6d6421603a","metadata":{},"outputs":[],"source":["def plot(COST,ACC):\n","    fig, ax1 = plt.subplots()\n","    color = 'tab:red'\n","    ax1.plot(COST, color=color)\n","    ax1.set_xlabel('epoch', color=color)\n","    ax1.set_ylabel('total loss', color=color)\n","    ax1.tick_params(axis='y', color=color)\n","\n","    ax2 = ax1.twinx()\n","    color = 'tab:blue'\n","    ax2.set_ylabel('accuracy', color=color)  # we already handled the x-label with ax1\n","    ax2.plot(ACC, color=color)\n","    ax2.tick_params(axis='y', color=color)\n","    fig.tight_layout()  # otherwise the right y-label is slightly clipped\n","\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"id":"7e3687a6-19aa-4b90-a4c3-4fa64ebc0d30","metadata":{},"outputs":[],"source":["def plot_embdings(my_embdings,name,vocab):\n","\n","\n","\n","\n","  fig = plt.figure()\n","  ax = fig.add_subplot(111, projection='3d')\n","\n","  # Plot the data points\n","  ax.scatter(my_embdings[:,0], my_embdings[:,1], my_embdings[:,2])\n","\n","  # Label the points\n","  for j, label in enumerate(name):\n","\n","      i=vocab.get_stoi()[label]\n","\n","      ax.text(my_embdings[j,0], my_embdings[j,1], my_embdings[j,2], label)\n","\n","  # Set axis labels\n","  ax.set_xlabel('X Label')\n","  ax.set_ylabel('Y Label')\n","  ax.set_zlabel('Z Label')\n","\n","  # Show the plot\n","  plt.show()"]},{"cell_type":"code","execution_count":null,"id":"373a2e0b-ec2f-47f9-a70e-9024e335f079","metadata":{},"outputs":[],"source":["def save_list_to_file(lst, filename):\n","    \"\"\"\n","    Save a list to a file using pickle serialization.\n","\n","    Parameters:\n","        lst (list): The list to be saved.\n","        filename (str): The name of the file to save the list to.\n","\n","    Returns:\n","        None\n","    \"\"\"\n","    with open(filename, 'wb') as file:\n","        pickle.dump(lst, file)\n","\n","def load_list_from_file(filename):\n","    \"\"\"\n","    Load a list from a file using pickle deserialization.\n","\n","    Parameters:\n","        filename (str): The name of the file to load the list from.\n","\n","    Returns:\n","        list: The loaded list.\n","    \"\"\"\n","    with open(filename, 'rb') as file:\n","        loaded_list = pickle.load(file)\n","    return loaded_list"]},{"cell_type":"markdown","id":"f682ad19-c93d-493b-adde-109573678d82","metadata":{},"source":["---\n"]},{"cell_type":"markdown","id":"9a2e0e12-bbec-40ab-987e-bda52253ff20","metadata":{},"source":["# Data Pipeline\n"]},{"cell_type":"markdown","id":"188ffeed-fde2-40ce-b16e-b28ab5393b4f","metadata":{},"source":["### Tokenizer\n"]},{"cell_type":"markdown","id":"1efd6bf4-7c78-4f36-932a-3e4b3dad4c67","metadata":{},"source":["A tokenizer takes an input a document and breaks it up into individual tokens. Now, you may wonder, what's a token?\n","This example may help you understand it better.\n","\n","Imagine a token as a puzzle piece of a jigsaw puzzle. Each word, number, or small part of a word is a token. When we tokenize a document, we break it into these puzzle pieces so that a computer can understand and work with the text more easily, just like how you solve a puzzle by arranging its pieces.\n"]},{"cell_type":"markdown","id":"0935c0ef-d6c3-4c47-b35f-40027232ecb0","metadata":{},"source":["First import the **```get_tokenizer```** function from **```torchtext.data.utils```**\n"]},{"cell_type":"code","execution_count":null,"id":"efb73a5f-7fc0-42c2-89a6-437e5a789616","metadata":{},"outputs":[],"source":["from torchtext.data.utils import get_tokenizer"]},{"cell_type":"markdown","id":"84f1e8b0-7d7c-4a15-9f9c-8eaac23d4c3a","metadata":{},"source":["Next, we generate the tokenizer, and set it to \"basic_english\". Setting to \"basic_english\" forces **`get_tokenizer`** to create a tokenizer that handles basic English text and splits that text into individual tokens based on spaces and punctuation marks. For additional details, refer to the [`pytorch` documentation](https://pytorch.org/text/stable/data_utils.html#get-tokenizer).\n"]},{"cell_type":"code","execution_count":null,"id":"ab75a870-476d-4072-9e26-69496bc576bb","metadata":{},"outputs":[],"source":["tokenizer = get_tokenizer(\"basic_english\")"]},{"cell_type":"markdown","id":"d153ce11-56c7-4ce6-9088-ab57dcb58af5","metadata":{},"source":["To get an understanding of how the \"basic_english\" tokenizer works, run the following example:\n"]},{"cell_type":"code","execution_count":null,"id":"f84efe91-9f94-40fc-875f-b16927c186bd","metadata":{},"outputs":[],"source":["tokens = tokenizer(\"You can now install TorchText using pip!\")\n","tokens"]},{"cell_type":"markdown","id":"f3d1fa1f-203a-4233-b37d-1af17c54b287","metadata":{},"source":["## Text Classification\n","Let's build a text classification model using PyTorch and torchtext to classify news articles into one of four categories: World, Sports, Business, and Sci/Tech.\n"]},{"cell_type":"markdown","id":"437126ac-bec8-45e6-b021-f5892348cdd1","metadata":{},"source":["### Introduction to the dataset\n","\n","The following introduces the dataset and provides context for some of the code that follows.\n","\n","Let's load the training split of the **`AG_NEWS`** dataset from **`torchtext`** and illustrate how we can work with a dataset in this format:\n"]},{"cell_type":"code","execution_count":null,"id":"b95f2b9f-a191-45c2-ad0d-f1683b140c2a","metadata":{},"outputs":[],"source":["train_iter= AG_NEWS(split=\"train\")"]},{"cell_type":"markdown","id":"b6d4523f-3d4a-4ae3-88c1-8d1a0810cca4","metadata":{},"source":["The **`AG_NEWS`** dataset in **`torchtext`** does not support direct indexing like a list or tuple. It is not a random access dataset but rather an iterable dataset that needs to be used with an iterator. This approach is more efficient for text data.\n"]},{"cell_type":"markdown","id":"90cc6824-35fd-40d1-be7f-d7c62914b158","metadata":{},"source":["The following code demonstrates how to work with an iterator. First, the iterator is created by wrapping `train_iter` inside the **`iter`** function. To retrieve the values from the iterator, you can use the **```next()```** function, which will yield the label index value and the text:\n"]},{"cell_type":"code","execution_count":null,"id":"b389897e-31ef-49b1-866f-bd45aa6a371f","metadata":{},"outputs":[],"source":["y,text= next(iter(train_iter))\n","print(y,text)"]},{"cell_type":"markdown","id":"0a869318-e28b-4544-8db3-a40fd699e965","metadata":{},"source":["We can find the true label from the label index value using a dictionary:\n"]},{"cell_type":"code","execution_count":null,"id":"a33d753d-3b31-48db-9b75-35f7bd810cc7","metadata":{},"outputs":[],"source":["ag_news_label = {1: \"World\", 2: \"Sports\", 3: \"Business\", 4: \"Sci/Tec\"}\n","ag_news_label[y]"]},{"cell_type":"markdown","id":"0d4949df-0f18-4f27-8dda-8da128a8431b","metadata":{},"source":["We can also count the total number of classes that appear in `train_iter` to confirm that there is at least one instance of each class in the training data:\n"]},{"cell_type":"code","execution_count":null,"id":"48a4847b-7215-40c6-9c86-72cbfb4e6934","metadata":{},"outputs":[],"source":["num_class = len(set([label for (label, text) in train_iter ]))\n","num_class"]},{"cell_type":"markdown","id":"bff3586b-9c2a-4a09-8552-bc9d9043ae08","metadata":{},"source":["Because our dataset is an iterable we will create a generator function called **```yield_tokens```** to apply the **```tokenizer```** to the text items in the dataset. The purpose of the generator function **```yield_tokens```** is to yield tokenized texts one at a time. Instead of processing the entire dataset and returning all the tokenized texts in one go, the generator function processes and yields each tokenized text individually as it is requested. The tokenization process is performed lazily, which means the next tokenized text is generated only when needed, saving memory and computational resources.\n"]},{"cell_type":"code","execution_count":null,"id":"d7935869-4ec7-4b05-9351-ce08faf5f3b6","metadata":{},"outputs":[],"source":["def yield_tokens(data_iter):\n","    for  _,text in data_iter:\n","        yield tokenizer(text)"]},{"cell_type":"markdown","id":"75d305cf-2fd3-4e1a-b698-f86e4f1dc45d","metadata":{},"source":["### Token Indices\n"]},{"cell_type":"markdown","id":"be06b83e-f2bf-4b4d-bded-b18bba1fbe0c","metadata":{},"source":["We would like to represent tokens as numbers because NLP algorithms can process and manipulate numbers more efficiently and quickly than text tokens. We will generate a mapping between tokens in our vocabulary and numbers using the  **```build_vocab_from_iterator```** function. The numbers to which tokens in the vocabulary are mapped are typically referred to as 'token indices' or simply 'indices'. These indices are effectively the numeric representations of tokens inside the vocabulary.\n","\n","The **```build_vocab_from_iterator```** function, when applied to a list of tokens, assigns a unique index to each token based on its position in the vocabulary. These indices serve as a way to represent the tokens in a numerical format that can be easily processed by machine learning models.\n","\n","For example, given a vocabulary with tokens [\"apple\", \"banana\", \"orange\"], the corresponding indices might be [0, 1, 2], where \"apple\" is represented by index 0, \"banana\" by index 1, and \"orange\" by index 2.\n"]},{"cell_type":"markdown","id":"501cb4b2-7de9-412c-864c-742140fbd45b","metadata":{},"source":["The following code builds our vocabulary by iterating over each (y, sentence) tuple in the corpus, tokenizing each sentance as we iterate through the dataset using **`yield_tokens`**. The tokenized sentences are then passed to  **```build_vocab_from_iterator```** which yields the token indices.\n"]},{"cell_type":"code","execution_count":null,"id":"9e19c830-ebda-405c-a026-471ab6f0f8f0","metadata":{},"outputs":[],"source":["vocabulary = build_vocab_from_iterator(yield_tokens(train_iter), specials=[\"<unk>\"])\n","vocabulary.set_default_index(vocabulary[\"<unk>\"])"]},{"cell_type":"markdown","id":"b9af4dc7-d46f-4adb-894e-a687d1988b30","metadata":{},"source":["The `vocabulary` object contains all the tokens in our vocabulary along with their numeric representations. For instance, the following prints the first 5 tokens in our vocabulary along with their token indices:\n"]},{"cell_type":"code","execution_count":null,"id":"1ef1b8c5-01f1-46ef-a894-04591fa6eaef","metadata":{},"outputs":[],"source":["for key in list(vocabulary.vocab.get_stoi())[:5]:\n","    print(key +': ' + str(vocabulary.vocab.get_stoi()[key]))"]},{"cell_type":"markdown","id":"c5452955-a368-4357-bf0e-262875f605fd","metadata":{},"source":["Note that the vocabulary includes some non-obvious words, like \"zzz\" or the common Polish first name \"Zygmunt\" lower-cased. We can get specific token indices by passing in a list of tokens:\n"]},{"cell_type":"code","execution_count":null,"id":"fb8c5ee6-976e-4220-bd35-876f936ec8e3","metadata":{},"outputs":[],"source":["vocabulary([\"zzz\",\"zygmunt\"])"]},{"cell_type":"markdown","id":"39cd71ba-2151-4e34-9be8-96663c596576","metadata":{},"source":["Although the above exercise showed how you can construct a vocabulary of tokens and generate associated token indices from a corpus of text, in what follows we will use GloVe word embeddings, which necessitate the use of the vocabulary used to train GloVe. As such, the vocabulary we will actually use is the following:\n"]},{"cell_type":"code","execution_count":null,"id":"adaa6e20-db1e-4821-91a9-294bfd9ff659","metadata":{},"outputs":[],"source":["# Note that GloVe embeddings are typically downloaded using:\n","#glove_embedding = GloVe(name=\"6B\", dim=100\n","# However, the GloVe server is typically down. The class below offers a workaround\n","\n","\n","class GloVe_override(Vectors):\n","    url = {\n","        \"6B\": \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/tQdezXocAJMBMPfUJx_iUg/glove-6B.zip\",\n","    }\n","\n","    def __init__(self, name=\"6B\", dim=100, **kwargs) -> None:\n","        url = self.url[name]\n","        name = \"glove.{}.{}d.txt\".format(name, str(dim))\n","        super(GloVe_override, self).__init__(name, url=url, **kwargs)\n","\n","glove_embedding = GloVe_override(name=\"6B\", dim=100)\n"]},{"cell_type":"code","execution_count":null,"id":"f5e703b0-6634-4f19-9e81-7fea66753c50","metadata":{},"outputs":[],"source":["# Setup GloVe vocabulary object\n","from torchtext.vocab import vocab\n","vocabulary = vocab(glove_embedding .stoi, 0,specials=('<unk>', '<pad>'))\n","vocabulary.set_default_index(vocabulary[\"<unk>\"])"]},{"cell_type":"markdown","id":"a7feb4c8-435d-4450-a358-be14b57e6782","metadata":{},"source":["Print the first 5 tokens in the GloVe vocabulary along with their token indices:\n"]},{"cell_type":"code","execution_count":null,"id":"594cb33e-ee3c-444d-9aa7-e4321f210dbe","metadata":{},"outputs":[],"source":["for key in list(vocabulary.vocab.get_stoi())[:5]:\n","    print(key +': ' + str(vocabulary.vocab.get_stoi()[key]))"]},{"cell_type":"markdown","id":"a83645b9-8fa7-43db-b0cf-dca88ffabd04","metadata":{},"source":["Get specific indices from the GloVe vocabulary by passing a list of tokens:\n"]},{"cell_type":"code","execution_count":null,"id":"f083ea68-bd08-45f9-9e80-d88599223650","metadata":{},"outputs":[],"source":["vocabulary([\"zzz\",\"zygmunt\"])"]},{"cell_type":"markdown","id":"97050ffc-66a6-4200-8033-e692c99c0d74","metadata":{},"source":["### Split the dataset\n"]},{"cell_type":"markdown","id":"5107006a-a381-4741-9258-896088561266","metadata":{},"source":["We can convert the dataset into map-style datasets and then perform a random split to create separate training and validation datasets. The training dataset will contain 95% of the samples, while the validation dataset will contain the remaining 5%. These datasets can be used for training and evaluating a machine learning model for text classification on the `AG_NEWS` dataset.\n"]},{"cell_type":"code","execution_count":null,"id":"f1f2421e-aef3-44c6-82bd-4531715eeac8","metadata":{},"outputs":[],"source":["# Split the dataset into training and testing iterators.\n","train_iter, test_iter = AG_NEWS()\n","\n","# Convert the training and testing iterators to map-style datasets.\n","train_dataset = to_map_style_dataset(train_iter)\n","test_dataset = to_map_style_dataset(test_iter)\n","\n","# Determine the number of samples to be used for training and validation (5% for validation).\n","num_train = int(len(train_dataset) * 0.95)\n","\n","# Randomly split the training dataset into training and validation datasets using `random_split`.\n","# The training dataset will contain 95% of the samples, and the validation dataset will contain the remaining 5%.\n","split_train_, split_valid_ = random_split(train_dataset, [num_train, len(train_dataset) - num_train])"]},{"cell_type":"markdown","id":"a72d1b28-7ecb-432b-bf85-b99285d0dcac","metadata":{},"source":["### Check if a compatible GPU is available\n"]},{"cell_type":"markdown","id":"44020eee-2b6a-4aa7-9655-e63cf75ab153","metadata":{},"source":["The following code checks if a CUDA-compatible GPU is available in the system using PyTorch, a popular deep learning framework. If a GPU is available, it assigns the device variable to \"cuda\" (which stands for CUDA, the parallel computing platform and application programming interface model developed by NVIDIA). If a GPU is not available, it assigns the device variable to \"cpu\" (which means the code will run on the CPU instead).\n"]},{"cell_type":"code","execution_count":null,"id":"66985511-bb38-43df-bca0-f2be848012d9","metadata":{},"outputs":[],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","device"]},{"cell_type":"markdown","id":"8ebb8019-3c30-4fe2-b60d-d0ff92077dfe","metadata":{},"source":["### Data Loader\n"]},{"cell_type":"markdown","id":"8fba53fa-25e2-4d05-9321-98f70ab7fc19","metadata":{},"source":["In this section we will define the data loader.\n","\n","\n","We will begin by defining two functions for text processing pipelines. The **`text_pipeline`** function tokenizes the input text and subsequently produces the token indices from **`vocabulary`**. The **`label_pipeline`** function merely guarantees that the labels commence at zero (it's worth noting that the raw labels in the **`AG_NEWS`** dataset start at 1). These text and label pipelines will be used to process the raw data strings derived from the dataset iterators.\n"]},{"cell_type":"code","execution_count":null,"id":"5314eed1-3032-4662-bdbe-345ed251efd6","metadata":{},"outputs":[],"source":["def text_pipeline(x):\n","  return vocabulary(tokenizer(x))\n","\n","def label_pipeline(x):\n","   return int(x) - 1"]},{"cell_type":"markdown","id":"1a37c5e4-22fe-47b8-ae02-5a144a586bce","metadata":{},"source":["In PyTorch, the **`collate_fn`** function is used in conjunction with data loaders to customize the way batches are created from individual samples. The provided code defines a `collate_batch` function in PyTorch, which is used with data loaders to customize batch creation from individual samples. It processes a batch of data, including labels and text sequences. It applies the `label_pipeline` and `text_pipeline` functions to preprocess the labels and texts, respectively. The processed data is then converted into PyTorch tensors and returned as a tuple containing the label tensor, text tensor, and offsets tensor representing the starting positions of each text sequence in the combined tensor. The function also ensures that the returned tensors are moved to the specified device (e.g., GPU) for efficient computation.\n"]},{"cell_type":"code","execution_count":null,"id":"f1852545-2bc5-4f21-871a-78a03603b023","metadata":{},"outputs":[],"source":["from torch.nn.utils.rnn import pad_sequence\n","\n","def collate_batch(batch):\n","    label_list, text_list = [], []\n","    for _label, _text in batch:\n","        label_list.append(label_pipeline(_label))\n","        text_list.append(torch.tensor(text_pipeline(_text), dtype=torch.int64))\n","\n","\n","    label_list = torch.tensor(label_list, dtype=torch.int64)\n","    text_list = pad_sequence(text_list, batch_first=True)\n","\n","\n","    return label_list.to(device), text_list.to(device)"]},{"cell_type":"markdown","id":"d3c4f5c4-cab7-4171-86bb-d512cbf627cd","metadata":{},"source":["We convert the dataset objects to a data loader by applying the collate function.\n"]},{"cell_type":"code","execution_count":null,"id":"cea39d99-0004-4ad1-a764-17c496deef7d","metadata":{},"outputs":[],"source":["BATCH_SIZE = 64\n","\n","train_dataloader = DataLoader(\n","    split_train_, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch\n",")\n","valid_dataloader = DataLoader(\n","    split_valid_, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch\n",")\n","test_dataloader = DataLoader(\n","    test_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch\n",")"]},{"cell_type":"markdown","id":"1d2c214e-1e99-455e-aeda-a800b9aefbab","metadata":{},"source":["We can see the output sequence when we have the label, text, and offsets for each batch.\n"]},{"cell_type":"code","execution_count":null,"id":"54e10135-7aca-4788-9dc9-e3620c809764","metadata":{},"outputs":[],"source":["label,seqence=next(iter(valid_dataloader ))\n"]},{"cell_type":"code","execution_count":null,"id":"6e268309-f521-4d4b-b533-edaca0e43f69","metadata":{},"outputs":[],"source":["embedding = nn.Embedding.from_pretrained(glove_embedding.vectors,freeze=False).to(device)"]},{"cell_type":"code","execution_count":null,"id":"0b266b4b-5051-45a7-95fe-fa29fbfb8e34","metadata":{},"outputs":[],"source":["word_embedding=embedding(seqence)"]},{"cell_type":"code","execution_count":null,"id":"e8d76845-496a-4d6c-af8a-28298f4b03d7","metadata":{},"outputs":[],"source":["word_embedding.mean(dim=1).shape"]},{"cell_type":"markdown","id":"d07da810-b9fc-491e-b13d-ab8d3a8ce9a0","metadata":{},"source":["### Neural Network\n"]},{"cell_type":"markdown","id":"8979b68a-7f0a-46e1-973b-5ea4ea8d5b6f","metadata":{},"source":["The following defines a neural network for text classification using an `Embedding` layer. Additionally, we have initialized the model using a specific method. When constructing the model, we utilize embedding layers loaded with pre-trained GloVe Word Vectors. The primary input parameters for the model include the number of target classes and the option to freeze the training layers.\n"]},{"cell_type":"code","execution_count":null,"id":"ff1b26ed-eaa6-4cac-b649-1365692bb052","metadata":{},"outputs":[],"source":["from torch import nn\n","\n","class TextClassifier(nn.Module):\n","    def __init__(self, num_classes,freeze=False):\n","        super(TextClassifier, self).__init__()\n","        self.embedding = nn.Embedding.from_pretrained(glove_embedding.vectors,freeze=freeze)\n","        # An example of adding additional layer: a linear layer and a ReLU activation\n","        self.fc1 = nn.Linear(in_features=100, out_features=128)\n","        self.relu1 = nn.ReLU()\n","        # The output layer that gives the final probabilities for the classes\n","        self.fc2 = nn.Linear(in_features=128, out_features=num_classes)\n","\n","    def forward(self, x):\n","        # Pass the input through the embedding layer\n","        x = self.embedding(x)\n","        # Here we use a simple mean pooling\n","\n","        x = torch.mean(x, dim=1)\n","        # Pass the pooled embeddings through the additional layers\n","        x = self.fc1(x)\n","        x = self.relu1(x)\n","        return self.fc2(x)"]},{"cell_type":"markdown","id":"ec3bfb02-9b9b-4632-bf94-7629bb5d2f34","metadata":{},"source":["###  Not freezing pre-trained weights\n","\n","We begin our analysis by unfreezing the pre-trained weights in the embedding layer:\n"]},{"cell_type":"code","execution_count":null,"id":"c7b7896a-a2c0-411f-b02f-7f392c2f0d11","metadata":{},"outputs":[],"source":["model=TextClassifier(num_classes=4,freeze=False)\n","model.to(device)"]},{"cell_type":"markdown","id":"8ae0cf27-b11c-4d90-afef-da306a7f9b03","metadata":{},"source":["We can obtain predicted labels from `model` using `predicted_label=model(text, offsets)` given input text and its corresponding offsets:\n"]},{"cell_type":"code","execution_count":null,"id":"c861cd20-f180-4f15-9cb8-ecf69ae69a5b","metadata":{},"outputs":[],"source":["model.eval()\n","predicted_label=model(seqence)"]},{"cell_type":"code","execution_count":null,"id":"839014cd-132d-494f-8620-03fcb70e697d","metadata":{},"outputs":[],"source":["predicted_label.shape"]},{"cell_type":"markdown","id":"4435ea23-15b4-4b13-94f9-4ce16e7f9135","metadata":{},"source":["We verify the output shape of our model. In this case, the model is trained with a mini-batch size of 64 samples. The output layer of the model produces 4 logits for each neuron, corresponding to the four classes in the classification task. We can also create a function to find the accuracy given a dataset.\n"]},{"cell_type":"code","execution_count":null,"id":"0eca4694-1829-4866-abdc-862f7d4db218","metadata":{},"outputs":[],"source":["def predict(text, model, text_pipeline):\n","    with torch.no_grad():\n","        text = torch.unsqueeze(torch.tensor(text_pipeline(text)),0).to(device)\n","\n","        output = model(text)\n","        return ag_news_label[output.argmax(1).item() + 1]"]},{"cell_type":"code","execution_count":null,"id":"2c531319-c926-4cfb-98ad-4a7578dc9df9","metadata":{},"outputs":[],"source":["predict(\"I like sports and stuff\", model, text_pipeline)"]},{"cell_type":"code","execution_count":null,"id":"533bb86a-e1fb-44c8-b147-1b358274bd90","metadata":{},"outputs":[],"source":["def evaluate(dataloader, model, device):\n","    model.eval()\n","    correct = 0\n","    total = 0\n","    with torch.no_grad():\n","        for label, text in dataloader:\n","            label, text = label.to(device), text.to(device)\n","            outputs = model(text)\n","            _, predicted = torch.max(outputs.data, 1)\n","            total += label.size(0)\n","            correct += (predicted == label).sum().item()\n","    accuracy = 100 * correct / total\n","    return accuracy"]},{"cell_type":"markdown","id":"3cdf3bb5-d2b4-4015-a29c-eb2ca7eae93b","metadata":{},"source":["The following evaluates the performance of the model. Note that, as of now, the model has no predictive power. This outcome is expected, considering that the model has not undergone any training yet.\n"]},{"cell_type":"code","execution_count":null,"id":"7ed46a21-6a67-4d8d-9898-dfd4d18c07f1","metadata":{},"outputs":[],"source":["evaluate(test_dataloader, model, device)"]},{"cell_type":"markdown","id":"364261c7-81a5-430a-8426-9786ada6584d","metadata":{},"source":["Let's now train our model.\n"]},{"cell_type":"code","execution_count":null,"id":"e4d403f3-16af-4529-a268-63d82032b0a0","metadata":{},"outputs":[],"source":["def train_model(model, optimizer, criterion, train_dataloader, valid_dataloader, epochs=100, model_name=\"my_modeldrop\"):\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    print(\"device: \",device)\n","    model = model.to(device)\n","    \n","    cum_loss_list = []\n","    acc_epoch = []\n","    best_acc = 0\n","    file_name = model_name\n","\n","    for epoch in tqdm(range(1, epochs + 1)):\n","        model.train()\n","        cum_loss = 0\n","        for _, (label, text) in enumerate(train_dataloader):\n","            label, text = label.to(device), text.to(device)\n","            \n","            optimizer.zero_grad()\n","            predicted_label = model(text)\n","            loss = criterion(predicted_label, label)\n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n","            optimizer.step()\n","            cum_loss += loss.item()\n","        print(\"Loss:\", cum_loss)\n","        \n","        cum_loss_list.append(cum_loss)\n","        acc_val = evaluate(valid_dataloader, model, device)\n","        acc_epoch.append(acc_val)\n","        \n","        if acc_val > best_acc:\n","            best_acc = acc_val\n","            print(f\"New best accuracy: {acc_val:.4f}\")\n","            #torch.save(model.state_dict(), f\"{model_name}.pth\")\n","\n","    #save_list_to_file(cum_loss_list, f\"{model_name}_loss.pkl\")\n","    #save_list_to_file(acc_epoch, f\"{model_name}_acc.pkl\")\n"]},{"cell_type":"markdown","id":"5c89ac8e-ad7c-40a2-a5f2-e45315579bfc","metadata":{},"source":["---\n"]},{"cell_type":"code","execution_count":null,"id":"d3b0fe66-e335-4b5f-a174-0c403f2af1a4","metadata":{},"outputs":[],"source":["LR=1\n","\n","criterion = torch.nn.CrossEntropyLoss()\n","optimizer = torch.optim.SGD(model.parameters(), lr=LR)\n","scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)"]},{"cell_type":"markdown","id":"c44a0f53-8365-4927-ab5f-43b8a4f29a02","metadata":{},"source":["\n","For your convenience, we’ve already trained the model for 100 epochs and saved it. This process typically takes around 25 minutes using a V100 GPU. To give you a taste of the training process, we’ve configured the model to train for just 1 epoch below; on cpu this takes around 8 minutes. However, please note that our conclusions about the model’s performance will be based on the previously saved 100-epoch model, not this brief 1-epoch run:\n"]},{"cell_type":"code","execution_count":null,"id":"c8665b90-8d40-40a0-b207-91fab57f9b98","metadata":{},"outputs":[],"source":["model_name=\"my_model_freeze_false\"\n","train_model(model, \n","            optimizer, \n","            criterion, \n","            train_dataloader, \n","            valid_dataloader, \n","            epochs=1, \n","            model_name=model_name)\n"]},{"cell_type":"markdown","id":"cbb70599-6ff9-40ca-b052-ce4deb71e6f1","metadata":{},"source":["We have the capability to upload the trained model along with comprehensive data on cumulative loss and average accuracy at each epoch.\n"]},{"cell_type":"markdown","id":"36c8517c-d5d9-4d26-8814-9f3db64bdb4a","metadata":{},"source":["We can plot the cost and accuracy for each epoch. We see that with just 100 epochs, we achieve an accuracy of over 80% on the validation data. You can increase the number of epochs to observe further improvement.\n"]},{"cell_type":"code","execution_count":null,"id":"6201051e-517e-427a-a151-0636253f6931","metadata":{},"outputs":[],"source":["from urllib.request import urlopen\n","\n","cum_loss_list=pickle.load(urlopen('https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/Zc8G0LmpcVbwkhG68GYxVg/my-model-freeze-false-loss.pkl'))\n","acc_epoch=pickle.load(urlopen('https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/DC_i68jjqbGxOmnJzQfTiw/my-model-freeze-false-acc.pkl'))\n","\n","plot(cum_loss_list,acc_epoch)"]},{"cell_type":"markdown","id":"23a51b95-8e3a-40a7-8f05-05845c9a57fc","metadata":{},"source":["The following loads the fine-tuned model:\n"]},{"cell_type":"code","execution_count":null,"id":"92a107fc-b0f7-489a-8086-66dae7d1827e","metadata":{},"outputs":[],"source":["import io\n","\n","model=TextClassifier(num_classes=4,freeze=False)\n","model.to(device)\n","\n","urlopened = urlopen('https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/uGC04Pom651hQs1XrZ0NsQ/my-model-freeze-false.pth')\n","stream = io.BytesIO(urlopened.read())  # implements seek()\n","state_dict = torch.load(stream, map_location=device)\n","model.load_state_dict(state_dict)"]},{"cell_type":"markdown","id":"f93221c8-e9e5-4808-be94-11f82d9eaa4c","metadata":{},"source":["Let's evaluate the model on the test dataset\n"]},{"cell_type":"code","execution_count":null,"id":"f769f5fb-44b9-4bdd-b611-ef0eb1598794","metadata":{},"outputs":[],"source":["evaluate(test_dataloader , model, device)"]},{"cell_type":"markdown","id":"ed00fe16-c85c-4068-997c-661c1d8d2e27","metadata":{},"source":["The model with unfrozen embedding weights achieves an accuracy of above 80% on the test data.\n"]},{"cell_type":"markdown","id":"66222174-5584-4c8f-af91-e43274cc3023","metadata":{},"source":["### Freezing pre-trained weights\n","The following model freezes the pre-trained weights, focusing training efforts exclusively on the layers situated above the embeddings. This approach leverages the stability of pre-existing knowledge while adapting the higher layers to our specific dataset\n"]},{"cell_type":"code","execution_count":null,"id":"2753efa3-3261-4702-b54e-6e970eb925a3","metadata":{},"outputs":[],"source":["model_t=TextClassifier(num_classes=4,freeze=True)\n","model_t.to(device)"]},{"cell_type":"code","execution_count":null,"id":"8f2bf8ae-e282-48aa-b348-4a5df29fb618","metadata":{},"outputs":[],"source":["LR=1\n","criterion = torch.nn.CrossEntropyLoss()\n","optimizer = torch.optim.SGD(model_t.parameters(), lr=LR)\n","scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)"]},{"cell_type":"markdown","id":"cb5021fb-3bfa-466e-b0ef-3a367969ee6c","metadata":{},"source":["The following trains the model. Note that because the embedding layer is frozen, the training takes about 15 minutes to run 100 epochs on a V100 GPU. This is significantly shorter than the 25 minutes it takes to train the model with the embedding layer unfrozen. In order to give you a taste of how the training works, the following trains the model on just 1 epoch:\n"]},{"cell_type":"code","execution_count":null,"id":"5fdeea20-a72a-4ee7-9051-df7cdc6b3cbb","metadata":{},"outputs":[],"source":["model_name=\"my_model_freeze_true\"\n","train_model(model_t, \n","            optimizer, \n","            criterion, \n","            train_dataloader, \n","            valid_dataloader, \n","            epochs=1, \n","            model_name=model_name)"]},{"cell_type":"markdown","id":"92e03b0f-d314-4612-a75c-f31b7cfbbd64","metadata":{},"source":[" We can plot the cost and accuracy for each epoch:\n"]},{"cell_type":"code","execution_count":null,"id":"075ada50-3d69-4384-a637-63f724fc1ce7","metadata":{},"outputs":[],"source":["cum_loss_list=pickle.load(urlopen('https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/7pehjCrIU7uW9eviumecqQ/my-model-freeze-true-loss.pkl'))\n","acc_epoch=pickle.load(urlopen('https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/iaJp4DYtS9unbwR6zvBjNA/my-model-freeze-true-acc.pkl'))\n","\n","plot(cum_loss_list,acc_epoch)"]},{"cell_type":"markdown","id":"e4925cba-e5b3-436e-a5d7-2240bbe3d2fc","metadata":{},"source":["The following loads the fine-tuned model:\n"]},{"cell_type":"code","execution_count":null,"id":"3356fa7a-2033-4fd0-a759-17aae1294d9c","metadata":{},"outputs":[],"source":["model_t=TextClassifier(num_classes=4,freeze=True)\n","model_t.to(device)\n","\n","urlopened = urlopen('https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/pYe-aNUEhAWGmKXcOBxTFw/my-model-freeze-true.pth')\n","stream = io.BytesIO(urlopened.read())  # implements seek()\n","state_dict = torch.load(stream, map_location=device)\n","model_t.load_state_dict(state_dict)"]},{"cell_type":"markdown","id":"3837cc56-d589-492c-ba3b-efd3c5c4e4ac","metadata":{},"source":["And the following evaluates the model's performance on test data:\n"]},{"cell_type":"code","execution_count":null,"id":"d78d1b3e-c250-430c-a904-c10cd521381f","metadata":{},"outputs":[],"source":["evaluate(test_dataloader , model_t, device)"]},{"cell_type":"markdown","id":"6d55b89a-dfec-4fe0-bbe9-3855c207e3e9","metadata":{},"source":["As you can seem the model with embeddings frozen performs significantly worse, achieving an accuracy of between 50% and 60% after 100 epochs.\n","\n","This result is somewhat predictable given the fact that we have only one hidden layer with a ReLU activation. Perhaps the model was too simple, and there weren't enough parameters to tune? We can test this hypothesis by training a substantially more complicated model. We do this in the next section.\n"]},{"cell_type":"markdown","id":"6c0393ca-4be3-4284-81c0-cc9190b20b4b","metadata":{},"source":["### Freezing pre-trained weights with a complicated model\n"]},{"cell_type":"markdown","id":"60359d63-fa67-49ec-9524-1f2c8f27800f","metadata":{},"source":["The following describes a neural network that has significantly more layers and weights compared to the `TextClassifier` model. The selection of the number of hidden layers and the quantity of features within these layers was determined through experimentation. The goal was to ensure that the training duration of this model would be approximately the same as the original model (with its embedding layer unfrozen), which is around 25 minutes.\n"]},{"cell_type":"code","execution_count":null,"id":"90c4eb1e-4fdf-4394-83ba-3444f9f4c2ec","metadata":{},"outputs":[],"source":["class TextClassifier2(nn.Module):\n","    def __init__(self, num_classes,freeze=False):\n","        super(TextClassifier2, self).__init__()\n","        self.embedding = nn.Embedding.from_pretrained(glove_embedding.vectors,freeze=freeze)\n","        # An example of adding additional layers: linear layers and ReLU activations\n","        self.fc1 = nn.Linear(in_features=100, out_features=4096)\n","        self.relu1 = nn.ReLU()\n","        self.fc2 = nn.Linear(in_features=4096, out_features=4096)\n","        self.relu2 = nn.ReLU()\n","        self.fc3 = nn.Linear(in_features=4096, out_features=4096)\n","        self.relu3 = nn.ReLU()\n","        self.fc4 = nn.Linear(in_features=4096, out_features=4096)\n","        self.relu4 = nn.ReLU()\n","        self.fc5 = nn.Linear(in_features=4096, out_features=4096)\n","        self.relu5 = nn.ReLU()\n","        self.fc6 = nn.Linear(in_features=4096, out_features=4096)\n","        self.relu6 = nn.ReLU()\n","        # The output layer that gives the final probabilities for the classes\n","        self.fc7 = nn.Linear(in_features=4096, out_features=num_classes)\n","\n","    def forward(self, x):\n","        # Pass the input through the embedding layer\n","        x = self.embedding(x)\n","        # Here we use a simple mean pooling\n","        x = torch.mean(x, dim=1)\n","        # Pass the pooled embeddings through the additional layers\n","        x = self.fc1(x)\n","        x = self.relu1(x)\n","        x = self.fc2(x)\n","        x = self.relu2(x)\n","        x = self.fc3(x)\n","        x = self.relu3(x)\n","        x = self.fc4(x)\n","        x = self.relu4(x)\n","        x = self.fc5(x)\n","        x = self.relu5(x)\n","        x = self.fc6(x)\n","        x = self.relu6(x)\n","        return self.fc7(x)"]},{"cell_type":"code","execution_count":null,"id":"b9ae7d2e-23a8-483f-99ab-9abae38f1163","metadata":{},"outputs":[],"source":["model_t2=TextClassifier2(num_classes=4,freeze=True)\n","model_t2.to(device)"]},{"cell_type":"code","execution_count":null,"id":"9e71dc83-1b6a-4563-865a-6a2b7e7fd5a0","metadata":{},"outputs":[],"source":["LR=1\n","criterion = torch.nn.CrossEntropyLoss()\n","optimizer = torch.optim.SGD(model_t2.parameters(), lr=LR)\n","scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)"]},{"cell_type":"markdown","id":"1dacf209-9fdb-43e0-ad9c-b2b5c78ac463","metadata":{},"source":["The following trains the model, which we will not do here because it would take a long time to run on CPU:\n"]},{"cell_type":"code","execution_count":null,"id":"9dca0b10-a33b-4ca8-9b65-9309c278556b","metadata":{},"outputs":[],"source":["model_name=\"my_model_freeze_true2\"\n","'''\n","train_model(model_t2, \n","            optimizer, \n","            criterion, \n","            train_dataloader, \n","            valid_dataloader, \n","            epochs=1, \n","            model_name=model_name)\n","'''"]},{"cell_type":"code","execution_count":null,"id":"67c0f5c2-0a1b-4103-ab22-f2dca0c20b70","metadata":{},"outputs":[],"source":["cum_loss_list=pickle.load(urlopen('https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/cQ_HdoQ_cMPj7xstATwKBA/my-model-freeze-true2-loss.pkl'))\n","acc_epoch=pickle.load(urlopen('https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/6ikmqnCk3OVVF3wutoBZwA/my-model-freeze-true2-acc.pkl'))\n","\n","plot(cum_loss_list,acc_epoch)"]},{"cell_type":"markdown","id":"04f6e896-9a97-419a-b6e4-2144151f9ed5","metadata":{},"source":["The following loads the fine-tuned model:\n"]},{"cell_type":"code","execution_count":null,"id":"71c8e229-1b88-4d1e-91d7-bc31e312f1b3","metadata":{},"outputs":[],"source":["model_t2=TextClassifier2(num_classes=4,freeze=True)\n","model_t2.to(device)\n","\n","urlopened = urlopen('https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/8G4X025HlxCkHruVEesasg/my-model-freeze-true2.pth')\n","stream = io.BytesIO(urlopened.read())  # implements seek()\n","state_dict = torch.load(stream, map_location=device)\n","model_t2.load_state_dict(state_dict)"]},{"cell_type":"markdown","id":"8c345bcf-ee08-4448-91ef-c3da6b5cf5fa","metadata":{},"source":["Let's evaluate the model's performance on the test data:\n"]},{"cell_type":"code","execution_count":null,"id":"9bf8b2a6-7706-438e-9ac5-76696e8592ff","metadata":{},"outputs":[],"source":["evaluate(test_dataloader , model_t2, device)"]},{"cell_type":"code","execution_count":null,"id":"8cabf40b-ad3e-431f-b476-8851e2c9fece","metadata":{},"outputs":[],"source":["article=\"\"\"Canada navigated a stiff test against the Republic of Ireland on a rain soaked evening in Perth, coming from behind to claim a vital 2-1 victory at the Women’s World Cup.\n","Katie McCabe opened the scoring with an incredible Olimpico goal – scoring straight from a corner kick – as her corner flew straight over the despairing Canada goalkeeper Kailen Sheridan at Perth Rectangular Stadium in Australia.\n","Just when Ireland thought it had safely navigated itself to half time with a lead, Megan Connolly failed to get a clean connection on a clearance with the resulting contact squirming into her own net to level the score.\n","Minutes into the second half, Adriana Leon completed the turnaround for the Olympic champion, slotting home from the edge of the area to seal the three points.\"\"\""]},{"cell_type":"code","execution_count":null,"id":"c49f2605-22dc-4a2a-aa39-f8227c73710d","metadata":{},"outputs":[],"source":["result = predict(article, model, text_pipeline)\n","\n","markdown_content = f'''\n","<div style=\"background-color: lightgray; padding: 10px;\">\n","    <h3>{article}</h3>\n","    <h4>The category of the news article: {result}</h4>\n","</div>\n","'''\n","\n","md(markdown_content)"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.9"},"prev_pub_hash":"0559b059a1e5758f1070533eb2eeac5e1fdbcd41d3f7205fd154fc5f608cb66d"},"nbformat":4,"nbformat_minor":4}
